<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="generator" content="litedown 0.7">
<title>Introduction to bioacoustics</title>
<style type="text/css">
body {
  font-family: sans-serif;
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
  print-color-adjust: exact;
  -webkit-print-color-adjust: exact;
}
body, .abstract, code, .footnotes, footer, #refs, .caption { font-size: .9em; }
li li { font-size: .95em; }
ul:has(li > input[type="checkbox"]) { list-style: none; padding-left: 1em; }
*, :before, :after { box-sizing: border-box; }
a { color: steelblue; }
pre, img { max-width: 100%; }
pre { white-space: pre-wrap; word-break: break-word; }
pre code { display: block; padding: 1em; overflow-x: auto; }
code { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; }
:not(pre, th) > code, code[class], div > .caption { background: #f8f8f8; }
pre > code:is(:not([class]), .language-plain, .language-none, .plain), .box, .figure, .table { background: inherit; border: 1px solid #eee; }
pre > code {
  &.message { border-color: #9eeaf9; }
  &.warning { background: #fff3cd; border-color: #fff3cd; }
  &.error { background: #f8d7da; border-color: #f8d7da; }
}
.fenced-chunk { border-left: 1px solid #666; }
.code-fence {
  opacity: .4;
  border: 1px dashed #666;
  border-left: 2px solid;
  &:hover { opacity: inherit; }
}
.box, .figure, .table, table { margin: 1em auto; }
div > .caption { padding: 1px 1em; }
.figure { p:has(img, svg), pre:has(svg) { text-align: center; } }
.flex-col { display: flex; justify-content: space-between; }
table {
  &:only-child:not(.table > *) { margin: auto; }
  th, td { padding: 5px; font-variant-numeric: tabular-nums; }
  thead, tfoot, tr:nth-child(even) { background: whitesmoke; }
  thead th { border-bottom: 1px solid #ddd; }
  &:not(.datatable-table) {
    border-top: 1px solid #666;
    border-bottom: 1px solid #666;
  }
}
blockquote {
  color: #666;
  margin: 0;
  padding: 1px 1em;
  border-left: .5em solid #eee;
}
hr, .footnotes::before { border: 1px dashed #ddd; }
.frontmatter { text-align: center; }
#TOC {
  a { text-decoration: none; }
  ul { list-style: none; padding-left: 1em; }
  & > ul { padding: 0; }
  ul ul { border-left: 1px solid lightsteelblue; }
}
.body h2 { border-bottom: 1px solid #666; }
.body .appendix, .appendix ~ h2 { border-bottom-style: dashed; }
.main-number::after { content: "."; }
span[class^="ref-number-"] { font-weight: bold; }
.ref-number-fig::after, .ref-number-tab::after { content: ":"; }
.cross-ref-chp::before { content: "Chapter "; }
.cross-ref-sec::before { content: "Section "; }
.cross-ref-fig::before, .ref-number-fig::before { content: "Figure "; }
.cross-ref-tab::before, .ref-number-tab::before { content: "Table "; }
.cross-ref-eqn::before, .MathJax_ref:has(mjx-mtext > mjx-c + mjx-c)::before { content: "Equation "; }
.abstract, #refs {
  &::before { display: block; margin: 1em auto; font-weight: bold; }
}
.abstract::before { content: "Abstract"; text-align: center; }
#refs::before { content: "Bibliography"; font-size: 1.5em; }
.ref-paren-open::before { content: "("; }
.ref-paren-close::after { content: ")"; }
.ref-semicolon::after { content: "; "; }
.ref-and::after { content: " and "; }
.ref-et-al::after { content: " et al."; font-style: italic; }
.footnote-ref a {
  &::before { content: "["; }
  &::after { content: "]"; }
}
section.footnotes {
  margin-top: 2em;
  &::before { content: ""; display: block; max-width: 20em; }
}
.fade {
  background: repeating-linear-gradient(135deg, white, white 30px, #ddd 32px, #ddd 32px);
  opacity: 0.6;
}

@media print {
  body { max-width: 100%; }
  tr, img { break-inside: avoid; }
}
@media only screen and (min-width: 992px) {
  body:not(.pagesjs) pre:has(.line-numbers):not(:hover) { white-space: pre; }
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@xiee/utils@1.14.14/css/prism-xcode.min.css">
<script src="https://cdn.jsdelivr.net/combine/npm/katex@0.16.25/dist/katex.min.js,npm/katex@0.16.25/dist/contrib/auto-render.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/@xiee/utils@1.14.14/js/render-katex.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
</head>
<body>
<div class="frontmatter">
<div class="title"><h1>Introduction to bioacoustics</h1></div>
<div class="author"><h2>Jean Marchal, Francois Fabianek, Christopher Scott</h2></div>
<div class="date"><h3>2025-11-12</h3></div>
</div>
<div class="body">
<style>
body {
text-align: justify}
</style>
<p>\newpage</p>
<h1 id="chp:package-overview">Package overview</h1>
<p>The <strong>bioacoustics</strong> package contains all the necessary R functions to read audio recordings of various formats (<em>e.g.</em>, WAV, WAC, MP3, Zero-Crossing), filter noisy files, display audio signals, detect and extract automatically acoustic features for further analysis such as species identification based on classification of animal vocalisations. This package does not provide R functions to perform classification tasks. Other packages such as <strong>randomForest</strong>, <strong>extraTrees</strong>, <strong>mclust</strong>, or <strong>keras</strong> can be used in addition with the <strong>bioacoustics</strong> package to perform these tasks. Complementary functions for audio processing in R are also available in the <strong>tuneR</strong>, <strong>monitorR</strong>, <strong>warbleR</strong>, and <strong>seewave</strong> packages.</p>
<p>This package was originally built for bat bioacoustics, and thus the default arguments for all the available functions are set for bats. Users will have to set the arguments according to the group of animal vocalisations they which to study^[See Table 1 for examples of extraction arguments for different species groups]. The <strong>bioacoustics</strong> package can be used to display and extract acoustic features from recordings of birds, dolphins, frogs, insects, and other terrestrial or marine animals as long as their vocalisations can be recorded.</p>
<p>This package is structured around three main axes components containing R functions accessible to users and internal processes that will also be defined below for the sake of clarity and ease of comprehension. The three main components with the current functions are as follow:</p>
<p>The functions of this package are designed to address three main needs:</p>
<ul>
<li>
<p>The need to read, write and manipulate acoustic recordings:</p>
<ul>
<li><code>mp3_to_wav</code></li>
<li><code>read_audio</code> (<code>read_mp3</code>, <code>read_wac</code>, <code>read_wav</code>)</li>
<li><code>read_zc</code></li>
<li><code>resample</code></li>
<li><code>write_zc</code>
\newline</li>
</ul>
</li>
<li>
<p>The need to display what’s inside acoustic recordings, whether to plot or just extract metadata</p>
<ul>
<li><code>fspec</code></li>
<li><code>metadata</code></li>
<li><code>plot_zc</code></li>
<li><code>spectro</code>
\newline</li>
</ul>
</li>
<li>
<p>The need to analyse audio recordings in batch in search of specific vocalisations and extract acoustic features</p>
<ul>
<li><code>blob_detection</code></li>
<li><code>threshold_detection</code></li>
</ul>
</li>
</ul>
<h1 id="chp:audio-recordings">Audio recordings</h1>
<h2 id="sec:read">Read</h2>
<p>The <code>read_audio</code> function loads into R mono and stereo audio recordings with MP3, WAV, or WAC (proprietary format from Wildlife Acoustics) formats. <code>read_audio</code> is essentially an helper calling specific functions (<code>read_mp3</code>, <code>read_wac</code>, <code>read_wav</code>), to decode the various format of audio recordings.</p>
<h2 id="sec:metadata_1">Metadata</h2>
<p>The <code>metadata</code> function automatically extracts metadata (if present) embedded in audio recordings that were previously loaded into R with the <code>read_audio</code> function. It can also extract metadata from objects generated with the <code>threshold_detection</code> or <code>blob_detection</code> functions. The <code>metadata</code> function also extract GUANO metadata. GUANO stands for the “Grand Unified Acoustic Notation Ontology” and means to be a universal, extensible, open metadata format for bat (and non-bat) acoustic recordings. The R code necessary to extract GUANO metadata was originally provided by David Riggs under the MIT licence and is available <a href="https://github.com/riggsd/guano-r">here</a>.</p>
<h2 id="sec:display">Display</h2>
<p>The <code>fspec</code> function returns a matrix describing the power spectrum by frequency of a time wave using the Fast Fourrier Transform (FFT). Values are expressed in decibels (dB).</p>
<p>The <code>spectro</code> function generates a frequency spectrum representation of a time wave using the Fast Fourrier Transform (FFT).</p>
<h2 id="sec:manipulate">Manipulate</h2>
<p>The <code>mp3_to_wav</code> function converts MP3 to WAV files.</p>
<p>The <code>resample</code> function up- or down-sample the sample rate of a given audio recording.</p>
<h2 id="sec:analyse-and-extract-acoustic-features">Analyse and extract acoustic features</h2>
<h3 id="sec:threshold-detection">Threshold detection</h3>
<p>The <code>threshold_detection</code> function is a modified version of the Bat Bioacoustics software developed by Christopher Scott (2012). It combines several algorithms for detecting, filtering and segmenting audio events, and extracting audio features.</p>
<h4 id="sec:audio-event-detection">Audio event detection</h4>
<p>The proposed audio event detection function is a modified spectral sum function for peak-picking, with background noise reduction and echo suppression (Scott, 2012).</p>
<p>A recording typically contains broadband continuous background noise, and discrete pulses of acoustic energy expressed in dB. Any discrete portion of a recorded signal above the level of background noise in a recording is defined here as an ‘audio event’. Under this definition, an audio event may include animal vocalisations, call echoes (<em>e.g.</em>, for bats), and other noise sources either biotic (<em>e.g.</em>, stridulating insects) or abiotic (flowing water, rain, wind, and wind-induced vegetation noise).</p>
<p>Audio events can be detected in a recording by its energy content, and defining a threshold rule for selecting discrete portions of the recording containing energy (dB) above a fixed threshold. The spectral energy content of a recording is typicaly revealed by applying a Short-Time Fourier Transform (STFT), a technique that slides an analysis window of fixed size through the recording. The output from the windowed FFT analysis is a discrete set of values from which the locations of audio events can be identified through simple energy thresholding and peak-picking.</p>
<p>The two conventional methods of bioacoustic signal detection are the spectral peak and the spectral sum functions. For a signal \(x\) at time \(n\), \(X[n]\) is defined as its STFT, where \(|Xk[n]|\) is the spectral magnitude of the \(Kth\) FFT bin at \(n\). The spectral sum function calculates the sum of the STFT magnitudes (the total energy over the entire spectrum) at each consecutive window through the recording to create the following detection function:</p>
<p>$$Dsum[n] = sum|Xk[n]|$$</p>
<p>This conventional signal detection function depends on both levels of gain and background noise in the recording. This complicates the setting of a consistent detection threshold value, as recordings may be at different levels. Thus, requiring different threshold levels to detect the exact same audio events.</p>
<p>To resolve this problem, the detection function should be normalised for each recording by substracting the median value over all analysis windows from each detection function data point (Skowronski &amp; Fenton, 2009). The median value is an estimate of the noise floor of the recording, and this process of median offsetting allows the use of a fixed threshold parameter, that is then independent of the recording level. However, the process of normalisation requires that the entire recording must be acquired prior to processing, ruling out real-time analysis.</p>
<p>This is why the <code>threshold_detection</code> function estimates the noise floor using only past values of the recording (<em>i.e.</em>, fixed windows of previous analysis frames). Prior knowledge is therefore not required for the normalisation of the detection function, and real-time analysis remains a possibility. In addition, by estimating the noise floor locally, the function can dynamically react to changes in the magnitude of background noise winthin the recording. The size of the noise estimation window can be set using the <code>NWS</code> argument. We recommend a time window of 100 ms for short recordings (typically &lt; 1 min length) containing bat echolocation calls. However, we have found that a time window of 5000 ms is ideal in long recordings (typically &gt; 60 min length) containing bird vocalisations.</p>
<p>The noise floor is estimated and subtracted independently for each spectral bin of the FFT spectrum. Environmental noise and microphone self noise is rarely white in nature (<em>i.e.</em>, equal power at all frequencies), and is typically weighted more heavily at the low end of the frequency spectrum. Frequency-specific noise substraction can attenuate the noisier low-frequency regions of the spectrum more heavily than the higher frequency and lower noise parts. This process increases the sensitivity of the audio event detection at higher frequency regions of the spectrum, as signals in those regions consequently have a higher Signal to Noise Ratio (SNR).</p>
<p>A threshold function then selects candidate audio event locations from the detection function normalised outputs. This threshold function works by first marking the location at which the detection function crosses the trigger threshold level. The value (in dB above SNR) of this trigger threshold can be set using the <code>threshold</code> argument available in the <code>threshold_detection</code> function.</p>
<p>If the detection function does not fall below the threshold level (in dB above SNR) after a certain audio event duration, the noise estimation is resumed from this location. This duration threshold can be set using the <code>duration_thr</code> argument. The <code>duration_thr</code> length is usually set at the maximum audio event duration of the targeted group of species. For bats in eastern Canada this parameter is set by default at 80 ms. For calls of Bicknell’s Thrush (<em>Catharus bicknelii</em>), it is recommended to set it at 440 ms (see Table 1).</p>
<p>Candidate audio events are subsequently filtered using the following rule: if the duration of the detected audio event is less than \(x\) milliseconds (ms) it is removed. This minimum duration filter is set with the <code>min_dur</code> argument available in the <code>threshold_detection</code> function. This duration threshold is set to help remove spurious detections caused by transient noise. For bats in eastern Canada this parameter is set by default at 1.5 ms.</p>
<p>A temporal masking is also employed to reduce the influence of echoes on the detection function: an exponential decay curve is applied to the output of the detection function, which acts as an adaptive threshold. Echoes falling below the threshold do not contribute to the detection function as they are masked by the louder preceding audio event. The exponential decay curve is defined as:</p>
<p>$$F[n]=max(D[n],a*F[n-1]+(1-a)*D[n])$$</p>
<p>where \(F[n]\) is the threshold function, \(D[n]\) is the detection function and \(a\) is the exponential decay gain. This echo suppression function aims to reduce the false alarms caused by echoes exceeding the energy threshold that triggers the detection. Exponential Decay Gain (EDG) values &gt; 0.8 worked well in practice and are set by default at 0.996. The exponential decay gain can be set using the <code>EDG</code> argument in the <code>threshold_detection</code> function.</p>
<p>The detection function \(D[n]\) is generated by first summing all spectral magnitudes in frequency bands that are greater than both their local median values, and the temporal masking threshold. This value is considered to be the audio event content as defined above. A noise estimate is then taken as the sum of all local median values. Finally, the detection function is expressed as SNR in dB as:</p>
<p>$$SNR = \frac{signal}{noise}$$</p>
<p>The function uses by default a FFT window size of 256 points (which can be set using the <code>FFT_size</code> argument), with a Blackman-Harris 7-term window to reduce spectral leakage (Harris, 1978). Larger FFT windows produce finer frequency resolution (<em>i.e.</em>, an increased number of FFT bins, each with a narrower frequency span), but increase computation time and reduce temporal resolution due to Gabor’s uncertainty principle (Gabor, 1946). The overlap between two consecutives FFT windows is set by default at 87.5 % (which can be set using the <code>FFT_overlap</code> argument). The <strong>bioacoustics</strong> package rely on the <a href="http://www.fftw.org/">FFTW library</a> for efficient Fourier transforms.</p>
<p>The above mentioned functions has been tested and their performances evaluated on bat echolocation calls in Scott (2012). Tests on a real-world dataset of field recordings confirmed that the modified spectral sum function with background noise reduction and echo suppression outperformed the two conventional approaches (<em>i.e.</em>, spectral sum and spectral peak functions) in terms of accuracy. The noise substraction function performed well with all signal types across a broad range of thresholds, making it simpler to apply in practice where the recording content is not known a priori. As normalisation is applied in real time through local background noise substraction, the length of the recording to be analysed has no effect on function efficiency (Scott, 2012).</p>
<h4 id="sec:extraction-with-filtering-and-smoothing">Extraction with filtering and smoothing</h4>
<p>High Pass (HPF) and Low Pass filters (LPF) can be employed to reduce the amount of unwanted noise in the recording or to track particuliar audio events within a narrower frequency bandwith than the recording sampling rate. Frequencies below the HPF and above the LPF cutoff are greatly attenuated. These frequency filters can be set using the <code>HPF</code> and <code>LPF</code> arguments in the <code>threshold_detection</code> function. Note that these filters are described in this section, but are used by the <code>threshold_detection</code> function just after the conversion of the recording in the time / frequency domain using the Fast Fourier transform (FFT).</p>
<p>After the recording has been filtered and that the candidate audio events have been located, a second series of functions are employed to filter, extract and smooth the audio event in the time / frequency domain. The audio event extraction function starts a search for the next FFT windows from right-to-left (start) and from left-to-right (end) from each audio event centroid (<em>i.e.</em>, location of the audio event at the peak of its maximum energy content), in search for the start and the end of the audio event, respectivelly.</p>
<center>
![plot of chunk unnamed-chunk-1](figure/unnamed-chunk-1-1.png)
</center>
<p>The audio event extraction function relies on three different thresholds to either pursue or stop its search for the next FFT window: the energy content (expressed in dB), the SNR (dB), and the angle of the next FFT window. These thresholds can be set using the <code>start_thr</code>, <code>end_thr</code>, <code>SNR_thr</code> and <code>angle_thr</code> arguments respectively. The extraction stops as soon as the next FFT falls under the minimum threshold value of any of these thresholds. Note that the start and the end thresholds can be set independently, because audio events may have a louder part before (start) or after (end) their peak of maximum energy (dB). A function is then verifying on the temporal X-axis of the recording, if an audio event has been extracted twice, and if so, it keeps only the longest audio event.</p>
<p>The extracted audio event sequence is smoothed with a Kalman filtering function, whose parameters can also be set using the <code>KPE</code> (Kalman Process Error) and <code>KME</code> (Kalman Measurement Error) arguments. Another series of filtering thresholds are applied to the Kalman filtered outputs such as minimum duration (<code>min_dur</code>), maximum duration (<code>max_dur</code>), minimum time between two audio events (<code>minTBE</code>), maximum time between two audio events (<code>maxTBE</code>). Acoustic features are then extracted from the filtered frequency (Hz) and energy (dB) outputs.</p>
<h4 id="sec:acoustic-features-and-metadata_1">Acoustic features and metadata</h4>
<table>
<thead>
<tr>
<th align="left">Feature</th>
<th align="left">Unit</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">starting_time</td>
<td align="left">sec</td>
<td align="left">Location of the audio event in the recording</td>
</tr>
<tr>
<td align="left">duration</td>
<td align="left">ms</td>
<td align="left">Duration of the audio event</td>
</tr>
<tr>
<td align="left">freq_max_amp</td>
<td align="left">Hz</td>
<td align="left">Frequency of the maximum energy of the audio event</td>
</tr>
<tr>
<td align="left">freq_max</td>
<td align="left">Hz</td>
<td align="left">Highest frequency of the audio event</td>
</tr>
<tr>
<td align="left">freq_min</td>
<td align="left">Hz</td>
<td align="left">Lowest frequency of the audio event</td>
</tr>
<tr>
<td align="left">bandwidth</td>
<td align="left">Hz</td>
<td align="left">Difference between the highest (freq_max) and lowest (freq_min) frequencies</td>
</tr>
<tr>
<td align="left">freq_start</td>
<td align="left">Hz</td>
<td align="left">Frequency at the start of the audio event</td>
</tr>
<tr>
<td align="left">freq_center</td>
<td align="left">Hz</td>
<td align="left">Frequency at the half of the audio event</td>
</tr>
<tr>
<td align="left">freq_end</td>
<td align="left">Hz</td>
<td align="left">Frequency at the end of the audio event</td>
</tr>
<tr>
<td align="left">freq_knee</td>
<td align="left">Hz</td>
<td align="left">Frequency at which the slope is the steepest (knee)</td>
</tr>
<tr>
<td align="left">freq_c</td>
<td align="left">Hz</td>
<td align="left">Frequency at which the slope is the flatest (caracteristic frequency)</td>
</tr>
<tr>
<td align="left">freq_bw_knee_fc</td>
<td align="left">Hz</td>
<td align="left">Frequency bandwith between the knee and caracteristic frequency</td>
</tr>
<tr>
<td align="left">bin_max_energy</td>
<td align="left">Hz</td>
<td align="left">Frequency at the maximum of energy where the slope is the flatest</td>
</tr>
<tr>
<td align="left">pc_freq_max_amp</td>
<td align="left">Hz</td>
<td align="left">Location of the frequency with the maximum of energy</td>
</tr>
<tr>
<td align="left">pc_freq_min</td>
<td align="left">%</td>
<td align="left">Location of the minimum frequency</td>
</tr>
<tr>
<td align="left">pc_fmax</td>
<td align="left">%</td>
<td align="left">Location of the maximum frequency</td>
</tr>
<tr>
<td align="left">pc_knee</td>
<td align="left">%</td>
<td align="left">Location of the frequency at which the slope is the steepest</td>
</tr>
<tr>
<td align="left">temp_bw_knee_fc</td>
<td align="left">%</td>
<td align="left">Temporal bandwith between the knee and caracteristic frequency</td>
</tr>
<tr>
<td align="left">slope</td>
<td align="left">ms</td>
<td align="left">Raw slope estimate (frequency bandwith against duration)</td>
</tr>
<tr>
<td align="left">kalman_slope</td>
<td align="left">Hz / ms</td>
<td align="left">Smoothed slope estimate after Kalman filtering</td>
</tr>
<tr>
<td align="left">curve_pos_start</td>
<td align="left">Hz / ms</td>
<td align="left">Slope estimate at the begining of the audio event</td>
</tr>
<tr>
<td align="left">curve_pos_end</td>
<td align="left">Hz / ms</td>
<td align="left">Slope estimate at the end of the audio event</td>
</tr>
<tr>
<td align="left">curve_neg</td>
<td align="left">Hz / ms</td>
<td align="left">Slope negative antropy</td>
</tr>
<tr>
<td align="left">mid_offset</td>
<td align="left">dB</td>
<td align="left">Mid-offset</td>
</tr>
<tr>
<td align="left">snr</td>
<td align="left">dB</td>
<td align="left">Signal to noise ratio</td>
</tr>
<tr>
<td align="left">harmonic_distortion</td>
<td align="left">dB</td>
<td align="left">Level of harmonic distortion</td>
</tr>
<tr>
<td align="left">smoothness</td>
<td align="left"></td>
<td align="left">Time / frequency regularity</td>
</tr>
</tbody>
</table>
<p>The metadata embedded in the WAV file header can be extracted by setting the <code>metadata</code> argument to <code>TRUE</code>. Metadata can then be extracted from the object produced by <code>threshold_detection</code> using the <code>metadata</code> function.</p>
<p>The detection and extraction settings used with the <code>threshold_detection</code> and <code>blob_detection</code> functions can be saved as metadata by setting the <code>settings</code> argument to <code>TRUE</code>.</p>
<h3 id="sec:blob-detection">Blob detection</h3>
<p>The <code>blob_detection</code> function is a modified version of the Bat Classify software developed by Christopher Scott (2014). It combines several image processing, filtering and image feature extraction.</p>
<h4 id="sec:audio-event-detection-and-extraction">Audio event detection and extraction</h4>
<p>A recording typically contains broadband continuous background noise, and discrete pulses of acoustic energy expressed in dB. Any discrete portion of a recorded signal above the level of background noise in a recording is defined here as an ‘audio event’. Under this definition, an audio event may include animal vocalisations, call echoes (<em>e.g.</em>, for bats), and other noise sources, either biotic (<em>e.g.</em>, stridulating insects) or abiotic (flowing water, rain, wind, and wind-induced vegetation noise).</p>
<p>The spectral energy content of a recording is typicaly revealed by applying a Short-Time Fourier Transform (STFT). This technique slides an analysis window of fixed size through the recording. The output from the windowed FFT analysis is a discrete set of values from which the locations of the audio events can be identified through simple thresholding after background noise substraction (Scott, 2012).</p>
<p>The <code>blob_detection</code> function uses by default a FFT window size of 256 points (can be changed using the <code>FFT_size</code> argument), with a Blackman Harris 4-term window to reduce spectral leakage (Harris, 1978). Larger FFT windows produce finer frequency resolution (an increased number of FFT bins, each with a narrower frequency span), but increase computation time and reduce temporal resolution due to Gabor’s uncertainty principle (Gabor, 1946). The overlap between two consecutives FFT windows is set by default at 87.5 % (can be changed using the <code>FFT_overlap</code> argument). The <strong>bioacoustics</strong> package rely on the <a href="http://www.fftw.org/">FFTW library</a> for efficient Fourier transforms.</p>
<p>FFT values are used to display the spectrogram representation of the audio event for image processing. A blur is applied to smooth the spectrogram with a Gaussian function and the level of blur can be changed using the <code>blur</code> argument. This function reduces the level of Gaussian noise in the spectrogram. The background noise is then substracted. The background substraction uses a local average of the energy spectrum intensity to estimate the amount of noise to substract from each spectrogram. This estimate is then multiplied by the value of the <code>bg_substract</code> argument. By estimating the noise floor locally, the function can dynamically react to local variations in the noise floor intensity. A contrast boost function is finally applied on the normalised spectrogram values to increase the definition of the audio event contour against the background noise. The level of contrast boost can be set with the <code>contrast_boost</code> argument.</p>
<p>A ‘blob detection’ algorithm is applied on the processed spectrogram to detect a Region Of Interest (ROI). This function relies on the linear-time connected component labelling algorithm described in Chang et al. (2004), and adapted from Andrew Brampton (2011). The resulting function  simultaneously labels the connected FFT values (or ‘blob’) and their contours in the spectrogram. Both external, and possibly internal contours of each blob are detected and labeled. Labeling is done in a single pass over the spectrogram, while contour points are revisited more than once and up to four times (Chang et al., 2004). Moreover, the detection function extracts a blob in a sequential order of contour points, <em>i.e.</em> a ‘segment’, which is useful in the case of animal vocalisations.</p>
<p>The <code>blob_detection</code> function then discards any extracted segment which area is &lt; n pixels. This can be set using the <code>min_area</code> argument, and is set by default at 40 pixels to best extract bat echolocation calls. The values of these filtering parameters may be set differently to extract vocalisations from other group of animals. The segment is finally log-compressed, to convert magnitude into dB, before feature extraction.</p>
<p>A series of filtering functions are also applied to the segments such as minimum duration (<code>min_dur</code>), maximum duration (<code>max_dur</code>), minimum time between two segments (<code>min_TBE</code>), maximum time between two segments (<code>max_TBE</code>), to reduce the amount of unwanted noise or to track specific vocalisations within a narrower temporal window. Like temporal filters, High Pass (HPF) and Low Pass frequency filters (LPF) can also be set using the <code>HPF</code> and <code>LPF</code> arguments in the <code>blob_detection</code> function. Frequencies below the HPF and above the LPF cutoff are greatly attenuated. Acoustic features are then extracted from these filtered segments.</p>
<h4 id="sec:acoustic-features-and-metadata_2">Acoustic features and metadata</h4>
<p>The spectral and temporal moments are extracted from the audio event contour points (called ‘segment’), and a gradient histogram is built from the sequence of frequencies (Hz) and energy (expressed in dB) stored in each pixel values (for each extracted audio event). These sequences are available to users in addition to the other acoustic features from the function’s output.</p>
<table>
<thead>
<tr>
<th align="left">Feature</th>
<th align="left">Unit</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">starting_time</td>
<td align="left">sec</td>
<td align="left">Location of the audio event in the recording</td>
</tr>
<tr>
<td align="left">duration</td>
<td align="left">ms</td>
<td align="left">Duration of the audio event</td>
</tr>
<tr>
<td align="left">area</td>
<td align="left">pixels</td>
<td align="left">Estimated area of the audio event (in pixels)</td>
</tr>
<tr>
<td align="left">freq_centroid</td>
<td align="left">Hz</td>
<td align="left">Frequency at the centroid of the extracted audio event</td>
</tr>
<tr>
<td align="left">freq_bandwith</td>
<td align="left">Hz</td>
<td align="left">Difference between the highest (freq_max) and lowest (freq_min) frequencies</td>
</tr>
<tr>
<td align="left">freq_skew</td>
<td align="left">Hz</td>
<td align="left">Skewness of the frequency distribution of the audio event</td>
</tr>
<tr>
<td align="left">freq_kurtosis</td>
<td align="left">Hz</td>
<td align="left">Kurtosis of the frequency distribution of the audio event</td>
</tr>
<tr>
<td align="left">q</td>
<td align="left">Hz</td>
<td align="left">Centroid frequency divided by the frequency bandwith of the audio event</td>
</tr>
<tr>
<td align="left">freq_gini_impurity</td>
<td align="left">Hz</td>
<td align="left">Degree of smoothness of the frequency distribution of the audio event</td>
</tr>
<tr>
<td align="left">quant_2.5</td>
<td align="left">Hz</td>
<td align="left">2.5 percentile of the frequency distribution of the audio event</td>
</tr>
<tr>
<td align="left">quant_25</td>
<td align="left">Hz</td>
<td align="left">25 percentile of the frequency distribution of the audio event</td>
</tr>
<tr>
<td align="left">quant_50</td>
<td align="left">Hz</td>
<td align="left">50 percentile of the frequency distribution of the audio event</td>
</tr>
<tr>
<td align="left">quant_75</td>
<td align="left">Hz</td>
<td align="left">75 percentile of the frequency distribution of the audio event</td>
</tr>
<tr>
<td align="left">quant_97.5</td>
<td align="left">Hz</td>
<td align="left">97.5 percentile of the frequency distribution of the audio event</td>
</tr>
<tr>
<td align="left">freq_bw_95_ci</td>
<td align="left">Hz</td>
<td align="left">Frequency bandwith between the 97.5 and the 2.5 percentiles</td>
</tr>
<tr>
<td align="left">freq_bw_75_ci</td>
<td align="left">Hz</td>
<td align="left">Frequency bandwith between the 75 and the 25 percentiles</td>
</tr>
<tr>
<td align="left">temp_centroid</td>
<td align="left">ms</td>
<td align="left">Time at the centroid of the extracted audio event</td>
</tr>
<tr>
<td align="left">temp_bandwith</td>
<td align="left">ms</td>
<td align="left">Time difference between the begining and the end of the audio event</td>
</tr>
<tr>
<td align="left">temp_skew</td>
<td align="left">ms</td>
<td align="left">Skewness of the time distribution of the audio event</td>
</tr>
<tr>
<td align="left">temp_kurtosis</td>
<td align="left">ms</td>
<td align="left">Kurtosis of the time distribution of the audio event</td>
</tr>
<tr>
<td align="left">temp_gini_impurity</td>
<td align="left">ms</td>
<td align="left">Degree of smoothness of the temporal distribution of the audio event</td>
</tr>
<tr>
<td align="left">grad_centroid</td>
<td align="left"></td>
<td align="left">Gradient at the centroid of the extracted audio event</td>
</tr>
<tr>
<td align="left">grad_bandwith</td>
<td align="left"></td>
<td align="left">Gradient difference between the begining and the end of the audio event</td>
</tr>
<tr>
<td align="left">grad_skew</td>
<td align="left"></td>
<td align="left">Skewness of the gradient distribution of the audio event</td>
</tr>
<tr>
<td align="left">grad_kurtosis</td>
<td align="left"></td>
<td align="left">Kurtosis of the gradient distribution of the audio event</td>
</tr>
<tr>
<td align="left">grad_gini_impurity</td>
<td align="left"></td>
<td align="left">Degree of smoothness of the gradient distribution of the audio event</td>
</tr>
</tbody>
</table>
<h1 id="chp:zero-crossing">Zero-Crossing</h1>
<h2 id="sec:read-write">Read / Write</h2>
<p>The <code>read_zc</code> function can read into R a Zero-Crossing file generated from various bat recorders and by the Kaleidoscope software (Wildlife Acoustics, Inc). This function is a modified version of Peter Wilson’s <a href="http://www.peterwilson.id.au/Rcode/AnabatTools.R">Anabat Tools</a> (2013) and the <a href="http://users.lmi.net/corben/loader.txt">C source code</a> provided by Chris Corben.</p>
<p>The <code>write_zc</code> function writes or re-writes a Zero-Crossing file loaded with the <code>write_zc</code> function.</p>
<h2 id="sec:metadata_2">Metadata</h2>
<p>The <code>metadata</code> function automatically extracts file header from a Zero-Crossing file previously loaded with the <code>read_zc</code> function.</p>
<h2 id="sec:plot">Plot</h2>
<p>The <code>plot_zc</code> function can be used to plot the content of a Zero-Crossing file previously loaded with the <code>read_zc</code> function. This function is a modified version of Peter Wilson’s <a href="http://www.peterwilson.id.au/Rcode/AnabatTools.R">Anabat Tools</a> (2013).</p>
<h1 id="chp:datasets">Datasets</h1>
<h2 id="sec:myotis">Myotis</h2>
<p>The myotis dataset is a WAV file of 10 seconds, 16 bits, mono, with an original sampling rate at 500 kHz (time expanded by 10). It contains 11 echolocation calls of bats from the <em>Myotis</em> genus. The recording was made in United-Kingdom with a D500X bat detector from Pettersson Elektronik AB.</p>
<h2 id="sec:zc">zc</h2>
<p>The zc dataset is a Zero-Crossing file of 16384 dots containing a sequence of 24 echolocation calls of a hoary bat (<em>Lasiurus cinereus</em>). This ZC recording was made in the Gatineau Park, Quebec, eastern Canada, during the summer 2017 with a Walkabout bat detector from Titley Scientific.</p>
<h1 id="chp:references">References</h1>
<blockquote>
<p>Chang, F., Chen, C.-J., &amp; Lu, C.J. (2004). A linear-time component-labeling algorithm using contour tracing technique. computer vision and image understanding, 93(2), 206-220. <a href="https://doi.org/10.1016/j.cviu.2003.09.002">Link</a></p>
</blockquote>
<blockquote>
<p>Gabor, D. (1946). Theory of communication. Part 1: The analysis of information. Engineers-Part III: Radio and Communication.</p>
</blockquote>
<blockquote>
<p>Harris, F. J. (1978). On then use of windows with the discrete for harmonic analysis Fourier transform. Proceedings of the IEEE, 66(1), 51-83. <a href="https://doi.org/10.1109/PROC.1978.10837">Link</a></p>
</blockquote>
<blockquote>
<p>Scott, C. D. (2012). Automated techniques for bat echolocation call analysis.  PhD thesis. The University of Leeds Institute of Integrative and Comparative Biology, University of Leeds, Leeds, UK. 166 pages.</p>
</blockquote>
<blockquote>
<p>Skowronski, M. D., &amp; Fenton, M. B (2009). Detecting bat calls: An analysis of automated methods. Acta Chiropterologica, 11(1), 191-203. <a href="https://doi.org/10.3161/150811009X465811">Link</a></p>
</blockquote>
<h1 id="chp:see-also">See also</h1>
<pre><code class="language-r"># Package tutorial
vignette(&quot;tutorial&quot;, package = &quot;bioacoustics&quot;)
</code></pre>
<p>Table 1. Example of detection, filtering and extraction settings with the <code>threshold_detection</code> function for eastern canadian bats and calls of Bicknell’s Thrush (<em>Catharus bicknelii</em>).</p>
<table>
<thead>
<tr>
<th align="left">Parameters</th>
<th align="left">Eastern canadian bats</th>
<th align="left">Bicknell’s Thrush calls</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">threshold</td>
<td align="left">14</td>
<td align="left">12</td>
</tr>
<tr>
<td align="left">time_exp</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left">min_dur</td>
<td align="left">1.5</td>
<td align="left">140</td>
</tr>
<tr>
<td align="left">max_dur</td>
<td align="left">80</td>
<td align="left">440</td>
</tr>
<tr>
<td align="left">min_TBE</td>
<td align="left">30</td>
<td align="left">300</td>
</tr>
<tr>
<td align="left">max_TBE</td>
<td align="left">1000</td>
<td align="left">5000</td>
</tr>
<tr>
<td align="left">EDG</td>
<td align="left">0.996</td>
<td align="left">0.996</td>
</tr>
<tr>
<td align="left">LPF</td>
<td align="left">250000</td>
<td align="left">8000</td>
</tr>
<tr>
<td align="left">HPF</td>
<td align="left">16000</td>
<td align="left">2000</td>
</tr>
<tr>
<td align="left">FFT_size</td>
<td align="left">256</td>
<td align="left">256</td>
</tr>
<tr>
<td align="left">FFT_overlap</td>
<td align="left">0.875</td>
<td align="left">0.875</td>
</tr>
<tr>
<td align="left">start_thr</td>
<td align="left">40</td>
<td align="left">25</td>
</tr>
<tr>
<td align="left">end_thr</td>
<td align="left">20</td>
<td align="left">30</td>
</tr>
<tr>
<td align="left">SNR_thr</td>
<td align="left">10</td>
<td align="left">10</td>
</tr>
<tr>
<td align="left">angle_thr</td>
<td align="left">40</td>
<td align="left">45</td>
</tr>
<tr>
<td align="left">duration_thr</td>
<td align="left">80</td>
<td align="left">440</td>
</tr>
<tr>
<td align="left">NWS</td>
<td align="left">100</td>
<td align="left">1000</td>
</tr>
<tr>
<td align="left">KPE</td>
<td align="left">1e-05</td>
<td align="left">1e-05</td>
</tr>
<tr>
<td align="left">KME</td>
<td align="left">1e-05</td>
<td align="left">1e-04</td>
</tr>
</tbody>
</table>
</div>
</body>
</html>
